{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation and Comparison with Probabilistic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "from ipywidgets import interact\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import janitor\n",
    "from utils import ECDF\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives of Part 3 onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consolidate your knowledge of the Bayesian model building workflow and use probabilistic programming for parameter estimation;\n",
    "2. Use probabilistic programming for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian parameter estimation using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You've learnt the basics of Bayesian model building. The steps are\n",
    "1. To completely specify the model in terms of _probability distributions_. This includes specifying \n",
    "    - what the form of the sampling distribution of the data is _and_ \n",
    "    - what form describes our _uncertainty_ in the unknown parameters (This formulation is adapted from [Fonnesbeck's workshop](https://github.com/fonnesbeck/intro_stat_modeling_2017/blob/master/notebooks/2.%20Basic%20Bayesian%20Inference.ipynb) as Chris said it so well there).\n",
    "2. Calculate the _posterior distribution_.\n",
    "\n",
    "In the above, the form of the sampling distribution of the data was Binomial (described by the likelihood) and the uncertainty around the unknown parameter $p$ captured by the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to do the same using the **probabilistic programming language** PyMC3. There's _loads of cool stuff_ about PyMC3 and this paradigm, two of which are\n",
    "- _probabililty distributions_ are first class citizens, in that we can assign them to variables and use them intuitively to mirror how we think about priors, likelihoods & posteriors.\n",
    "- PyMC3 calculates the posterior for us: this is fancy math done for lazy programmers!\n",
    "\n",
    "Under the hood, PyMC3 will compute the posterior using a sampling based approach called Markov Chain Monte Carlo (MCMC) or Variational Inference. Check the [PyMC3 docs](https://docs.pymc.io/) for more on these. \n",
    "\n",
    "From this notebook onwards, we have prepared a series of examples that will show you how to perform inference and prediction in a variety of problems. Hopefully, you will see that at the end of the day, everything we do boils down to estimation of some kind, and by doing it in a Bayesian setting, we avoid many pitfalls that come from blindly following canned statistical procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Click-Through Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common experiment in tech data science is to test a product change and see how it affects a metric that you're interested in. \n",
    "\n",
    "Say that we don't think enough people are clicking a button on my website, but we hypothesize that it's because the button is a similar color to the background of the page, meaning they're a difficult time finding the buttong to click.\n",
    "\n",
    "We can serve up two pages, and randomly send some customers to each: the first the original page, the second a page that is identical, except that it has a button that is of higher contrast and see if more people click through. \n",
    "\n",
    "This is commonly referred to as an A/B test and the metric of interest is click-through rate (CTR), what proportion of people click through. We will use this example to help us build familiarity with PyMC3 mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Let's first load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = (\n",
    "    pd.read_csv('../data/clickthrough.csv', index_col=0)\n",
    "    .label_encode('group')  # FYI: this is a pyjanitor function    \n",
    ")\n",
    "ctr.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Build Model: Estimate $p$ for control group\n",
    "\n",
    "Now it's time to build our probability model. Noticing that our model of having a constant click-through rate resulting in click or not is a biased coin flip:\n",
    "\n",
    "- the sampling distribution is binomial and we need to encode this in the likelihood;\n",
    "- there is a single parameter $p$ that we need to describe the uncertainty around. \n",
    "    - Its value must be bound between 0 and 1, so we must use a probability distribution that takes on these bounds.\n",
    "    - Having not seen the data, we must express what we believe about the parameter $p$ -- this is nothing more than assigning credibility points to the number line for the value $p$. As a starter, we will show you how to express **\"equal credibility from 0 to 1\"**, by using the Uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "\n",
    "These are the ingredients for the model so let's now build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_df = ctr.query(\"group == 'control'\")\n",
    "\n",
    "with pm.Model() as model1_bernoulli:\n",
    "    p = pm._____________________\n",
    "    like = pm._____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alternative way to build this model, taking advantage of the fact that the sum of Bernoulli-distributed data follows a Binomial distribution. The syntax would look like this below, annotated with other modifications to guide you along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model of p_a\n",
    "with pm.Model() as model1_binomial:\n",
    "    # Prior on p\n",
    "    p = pm.Uniform('p')  # defaults for the uniform distribution are lower=0 and upper=1\n",
    "    # Binomial Likelihood\n",
    "    like = pm.Binomial(\n",
    "        'likelihood', \n",
    "        n=len(control_df), \n",
    "        p=p, \n",
    "        observed=len(control_df.query(\"clicks == 1\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the data have to be in a different shape, though. With the Bernoulli likelihood, we need every single success/failure to be recorded. With the Binomial likelihood, we only need the summary statistics: number of trials, and number of successes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from Posterior\n",
    "\n",
    "\n",
    "It's now time to sample from the posterior using PyMC3. You'll also plot the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with _________:\n",
    "    samples_bernoulli = pm._________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with _________:\n",
    "    samples_binomial = pm._________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Checking\n",
    "\n",
    "Now, let's use ArviZ to perform some visual diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Posterior plot for bernoulli model\n",
    "az._________________;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az._________________;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we get the same results using the two model formulations (Bernoulli- vs. binomial-distributed likelihoods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Interpret the posterior ditribution. What would your tell the non-technical manager of your growth team about the CTR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Build Model: Compare $p$ for control and test groups\n",
    "\n",
    "Having built the model for the control group, let's now extend it to compare the control and test groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Build Model\n",
    "\n",
    "Modify the first model (go ahead and copy/paste it here!) to estimate $p_{control}$ and $p_{test}$. Let's use Binomial likelihoods, for a bit of variety.\n",
    "\n",
    "**Hint:** You will probably want to have two different `p`s (e.g. `p_control` and `p_test`) as well as two likelihoods (`like_control`, and `like_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test_df, just as we created control_df above.\n",
    "test_df = ctr.query(\"group == 'test'\")\n",
    "\n",
    "# Give your model a variable name\n",
    "with pm.Model() as ___________:\n",
    "    # Copy p and likelihood for control group.\n",
    "    p_control = pm.Uniform('p_control')\n",
    "    like_control = pm.Binomial(\n",
    "        'like_control', \n",
    "        n=len(control_df), \n",
    "        p=p_control, \n",
    "        observed=len(control_df.query(\"clicks == 1\"))\n",
    "    )\n",
    "    \n",
    "    # Modify the above p and likelihood for test group.\n",
    "    \n",
    "\n",
    "    # We will also explicitly compute the difference between\n",
    "    # p_control and p_test. \n",
    "    # This shows you that we can do math on probability distributions!\n",
    "    p_diff = pm.Deterministic(\"p_diff\", p_test - p_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ___________:\n",
    "    trace = pm.___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which function?\n",
    "# Which trace?\n",
    "az.______________(____________, round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "How did the test group compare to the control group?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "Allen Downey wrote [a blog post](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html) that illustrates how all of statistical inference boils down into a single framework: test statistic, null model, and \"probability of unexpectedness\" (p-value) under that null model. All of the special statistical tests with fancy author names are merely particular cases of this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-along: Translating probability distributions into things that matter\n",
    "\n",
    "With Bayesian methods, we will go out on a limb to say that the p-value doesn't matter at all: posterior distributions are all we need, and we can use posterior distributions to do some cool stuff.\n",
    "\n",
    "Let's make use of the posterior distribution on differences to calculate how much money we expect to gain.\n",
    "\n",
    "If we know that our customers on average spend 25 USD after clicking, and 0 USD if they do not click, we can then simulate the distribution of expected increase of revenue over 1 million customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ___________\n",
    "plt.___________\n",
    "plt.xlabel(\"Expected revenue increase\")\n",
    "plt.ylabel(\"Cumulative probability\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Baseball Players\n",
    "\n",
    "We're now going to switch to a different dataset: that of baseball players and their batting stats. The goal of this analysis is to identify which player we want to target to make an offer to join our baseball team. \n",
    "\n",
    "To simplify the problem, we are going to make a decision on the basis of just batting average and their salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "1. Reinforce Binomial/bernoulli generative story.\n",
    "1. Illustrate how to use broadcasting to avoid writing for-loops. \n",
    "1. How to write a hierarchical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Let's load baseball player data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = (\n",
    "    pd.read_csv(\"../data/baseballdb/core/Batting.csv\")\n",
    "    .clean_names()\n",
    "    .query(\"yearid == 2016\")\n",
    "    .select_columns(['playerid', 'ab', 'h'])\n",
    "    .groupby('playerid').sum()\n",
    ")\n",
    "\n",
    "salaries = (\n",
    "    pd.read_csv(\"../data/baseballdb/core/Salaries.csv\")\n",
    "    .clean_names()\n",
    "    .query(\"yearid == 2016\")\n",
    "    .select_columns([\"playerid\", \"salary\"])\n",
    "    .groupby('playerid').mean()\n",
    ")\n",
    "\n",
    "data = (\n",
    "    players.join(salaries)\n",
    "    .dropna()\n",
    "    .reset_index().label_encode(\"playerid\").set_index(\"playerid\")\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-along: Build Model while avoiding for-Loops\n",
    "\n",
    "Previously, we were able to compare two groups by copy/pasting code. That's not an ideal way to construct our model here. We're going to show you how you can avoid for-loops by taking advantage of broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition\n",
    "\n",
    "Once again, we see the Bernoulli distribution data generation story at play here: we have a number of trials (`ab`), with a number of successes (`h`), from which we want to estimate a player-specific property: `p`, the probability of hitting a pitch.\n",
    "\n",
    "We are going to introduce a new distribution here: the `beta` distribution, which has the same desirable properties as the Uniform, but provides richer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your model a name.\n",
    "with pm.Model() as ___________:\n",
    "    # we construct a vector of beta distributions, one for each player.\n",
    "    p = pm.___________\n",
    "    like = pm.___________\n",
    "    \n",
    "    # Let's also construct a \"p per salary\" metric, based on their salary.\n",
    "    # We want highest p for lowest salary.\n",
    "    pps = pm.___________\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct model variable here\n",
    "with ___________:\n",
    "    # Give your trace a name\n",
    "    ___________ = pm.___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "Because we have 805 players, we're going to create a custom visualization that lets us scrub through our players' posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import SelectMultiple, HBox, VBox, Select, fixed\n",
    "\n",
    "def dict2tuple(d):\n",
    "    return list(*zip(*zip(d.items())))\n",
    "\n",
    "def inversedict(d):\n",
    "    return {v:k for k, v in d.items()}\n",
    "\n",
    "playerid_mapping = data['playerid_enc'].to_dict()\n",
    "playerid_inverse = inversedict(playerid_mapping)\n",
    "\n",
    "player_select = SelectMultiple(\n",
    "    options=dict2tuple(playerid_mapping),\n",
    "    value=(0,)\n",
    ")\n",
    "\n",
    "metric_select = Select(\n",
    "    options=['p', 'pps'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_posterior(player_encs: list, metric: str, trace):\n",
    "    fig, ax = plt.subplots()\n",
    "    for enc in player_encs:\n",
    "        x, y = ECDF(trace[metric][:, enc])\n",
    "        ax.plot(x, y, label=playerid_inverse[enc])\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"p per salary\")\n",
    "    ax.set_ylabel('cumulative probability')\n",
    "        \n",
    "    sns.despine()\n",
    "    \n",
    "    \n",
    "interact(scrub_posterior, player_encs=player_select, metric=metric_select, trace=fixed(trace_batting));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Finding our ideal player\n",
    "\n",
    "Using the visualization, find a bunch of players whom you think would be great to scout, based on their PPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical model\n",
    "\n",
    "**Discussion:** Is it reasonable that some players have a `p` (probability of hitting) value that ranges from as small as 0 to as high as 1? Under what assumption would this be reasonable? Under what assumption would this be *unreasonable*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-along: Model Definition\n",
    "\n",
    "To build a hierarchical model, we essentially need to define a prior on the parameters of the Beta distribution. These new priors now become \"hyperpriors\". How do we pick hyperpriors then? Some qualitative rules of thumb:\n",
    "\n",
    "1. Pick a distribution with the correct **support**. For example, if a parameter can/should never take negative values, then pick a distribution with only positive support.\n",
    "1. The distribution should be **weakly informative**. For example, rather than a uniform distribution hyperprior, one might choose a wide distribution (relative to the relevant scale of the parameters) that spans 1 order of magnitude difference, but still has uneven allocation of credibility. \n",
    "\n",
    "Use the PyMC3 distribution library to your advantage! There are pictures that show you the shapes of the distributions, which can be helpful in narrowing your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ___________:\n",
    "    a_prior = pm.___________\n",
    "    b_prior = pm.___________\n",
    "    \n",
    "    p = pm.___________\n",
    "    \n",
    "    like = pm.Binomial(\"like\", p=p, n=data['ab'], observed = data['h'])\n",
    "    \n",
    "    pps = pm.Deterministic(\"pps\", p / data['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ___________:\n",
    "    ___________ = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sampling happens, do you have any questions? Please feel free to ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands-on: Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(scrub_posterior, player_encs=player_select, metric=metric_select, trace=fixed(trace_hierarchical));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "\n",
    "1. What do you notice about the posterior distributions on `p`?\n",
    "1. What do you notice about the posterior distributions on `pps`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building Workflow\n",
    "\n",
    "**Things we did:**\n",
    "\n",
    "1. Get data into correct shape.\n",
    "1. Build naive model, followed by more complex model.\n",
    "1. Build a simple loss function into our model.\n",
    "\n",
    "\n",
    "**Things we didn't do:**\n",
    "\n",
    "1. Posterior predictive checks.\n",
    "\n",
    "**Bonus things:**\n",
    "\n",
    "1. Build custom posterior explorers. You can use your favourite framework of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
